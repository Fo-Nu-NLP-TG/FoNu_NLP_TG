# Gradient Descent & Backpropagation     

While backpropagation provides
a way to compute gradients for the network, stochastic gradient descent uses these
gradients to train the network.


- https://youtu.be/Ilg3gGewQ5U

- The embedding step is unsupervised and the learning step is typically supervised.

- One notable weakness of the original formulation of word2vec was disambiguation
- There was no way to distinguish between various uses of a word that may have different meanings depending on context, such as the case of homographsâ€”duck
(posture) versus duck (bird) or fair (a gathering) versus fair (just).